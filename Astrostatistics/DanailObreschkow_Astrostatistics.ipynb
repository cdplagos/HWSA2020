{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries and set RNG seed for this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "library(magicaxis)\n",
    "library(pracma)\n",
    "suppressMessages(library(cooltools))\n",
    "suppressMessages(library(ellipse)) # for plotting covariance ellipses\n",
    "suppressMessages(library(extraDistr)) # for truncated Normal distribution\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foreword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why \"astrostatistics\"?\n",
    "\n",
    "* Data are expensive\n",
    "\n",
    "* Astronomical data is gathered by observation rather than experimentation.\n",
    "\n",
    "* Data are fundamentally finite\n",
    "\n",
    "In appreciation of these motivations, I decided to dedicate this lecture to the topic of statistical *inference*, which, in the most general sense, is the art of finding a theoretical model describing some empirical observations. More specifically, statistical inference is the mathematical way to distinguish between good and bad models and to determine the free parameters of a model, given some data.\n",
    "\n",
    "In general, statistical inference is an *inductive* process as opposed to a *deductive* one, since it relies on a *finite* sample of observations to determine a model that can normally describe *infinitely* many cases. It is therefore unavoidable to make some axiomatic assumptions to constrain the problem enough that deductive logic can be applied.\n",
    "\n",
    "A good example for the inductive nature of inference is the search for physical laws: all known physical laws rely on a finite set of observations, yet apply to a *continuity* of infinitely many situations. For instance, Johannes Kepler proposed his revolutionary laws of planetary motion from a number of discrete position measurements of Mars, using only very limited additional data from the other five ancient planets (including Earth), collected by Tycho Brahe over a couple of decades. Desipite this finite set of data, Kepler's laws are formulated, as if they applied to *any* conceivable planet in the solar system at *any* time in the past and future. This is, of course, a logical leap: in principle, the laws of physics could suddently change any day or behave differently for not-yet-discovered planets. Similarly, Brahe's data were not good enough to distinguish between $R^3\\propto T^2$ (Kepler's third law) and $R^{3.001}\\propto T^{1.999}$. These considerations make it clear that the general character and exact formulation of physical laws do *not* follow from the data, but from *metaphysical* principles, such as the idea that the Universe obeys some *simplicity* (see Occam's razor), that it is built upon special *symmetries* (e.g. homogeneity and isotropy of space-time) and that it follows a mathematical *beauty* (e.g. integer exponents in Kepler's case).\n",
    "\n",
    "In summary, statistical inference can rarely determine the optimal model for some data in a fully deductive way. A powerful mathematical framework for inference should, however, be able to distinguish between models that make different predictions regarding the actually observed data; and it should be able to account for prior knowledge about the 'true' model. The mathematical framework that most directly incorporates these features is the 'Bayesian' framework, as it allows us to assign *probabilities* to models/model parameters, given the data. This part of the course is entirely dedicated to this Bayesian framework and associated tools (e.g. maximum likelihood and maxiumum posterior estimates).\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core idea of the Bayesian framework is to infer a model $M$ (or testing a hypothesis $H$) from a data set $D$ (also known as evidence $E$) by identifying the model and data with $A$ and $B$, respectively, in Bayes' theorem, $P(A|B)P(B)=P(B|A)P(A)$ (see part 3). We can thus write\n",
    "\\begin{equation}\n",
    "  P(M|D)=\\frac{P(D|M)P(M)}{P(D)},\n",
    "  \\label{eq:bayesMD}\n",
    "\\end{equation}\n",
    "where\n",
    "\n",
    "* $P(D|M)$ is the conditional probability of the data given the model, known as the *likelihood*;\n",
    "\n",
    "* $P(M)$ is the *prior probability* (often just called the *prior*), encoding our knowledge about the model before considering the data;\n",
    "\n",
    "* $P(M|D)$ is the *posterior probability* (often just called the *posterior*), i.e. the probability of the model, accounting for the data *and* prior knowledge;\n",
    "\n",
    "* $P(D)$ is called the *marginal likelihood*, because it is obtained by integrating ('marginalising') $P(D|M)$ over all models.\n",
    "\n",
    "Since $P(D)$ does not depend on the model, it has no bearing on the *relative* probabilities of different models. It only acts as a normalisation function and is often dropped in calculations.\n",
    "\n",
    "In many practical cases, the model $M$ to be inferred from the data $D$ belongs to a discrete or continuous family of models, specified by one or multiple discrete/continuous parameters. For instance, we might like to fit a normal distribution with parameters $\\mu$ (mean) and $\\sigma$ (standard deviation) to some data points.\n",
    "\n",
    "In the case of such parameterized models, it is common to rewrite the likelihood as $$\\mathcal{L}(\\theta;D)\\equiv P(D|\\theta)$$ and consider $\\mathcal{L}$ a function of $\\theta$ at fixed $D$ rather than the other way around. As a consequence, $\\mathcal{L}$ is not generally normalised, i.e. $\\int\\mathcal{L}\\,d\\theta\\neq1$, unlike standard probability density functions (PDFs); hence the symbol $\\mathcal{L}$ instead of $P$.\n",
    "\n",
    "The likelihood function *is the probability of the data $D$ assuming that the model is specified by the parameters $\\theta$*, which is *not* to be confused with the probability of $\\theta$ given $D$ (i.e. the *posterior*). This distinction is the reason that we use the semi-colon notation $(\\theta;D)$ for the likelihood instead of the vertical bar $(\\theta|D)$, normally reserved for conditional probabilities. Of course, in the scientific literature, all sorts of notations are used, so be prepared to get confused! It is also quite common to drop the data vector and simply write $\\mathcal{L}(\\theta)$.\n",
    "\n",
    "Following Bayes' theorem (Eq. \\ref{eq:bayesMD}) the posterior is related to the likelihood via,\n",
    "$$P(\\theta|D) \\propto \\mathcal{L}(\\theta;D)P(\\theta).$$\n",
    "Note that this equation uses a proportionality symbol instead of an equal symbol, because we have dropped the $\\theta$-independent marginalized likelihood $P(D)$. It is understood that $P(\\theta|D)$ has to be normalized, such that $\\int P(\\theta|D)\\,d\\theta=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: spaceship emergency\n",
    "\n",
    "Imagine you are on a spaceship and you need to land this spaceship on a planet, as a matter of emergency. Only two planets (A and B) are within reach and for the landing site to be hard enough, it has to rain on less than 50% of the days, on average, on the planet. You need to pick the best planet based on very limited data:\n",
    "\n",
    "* On planet A it rained on 2 of 3 randomly observed days.\n",
    "* On planet B it rained on 6 of 10 randomly observed days.\n",
    "\n",
    "Which planet to you pick?\n",
    "\n",
    "One might naively intuit that planet B is the smart choice, since the data suggest that it rains more frequently on planet A than on planet B ($2/3\\approx0.667$ vs $6/10=0.6$). However this answer is too quick. In fact, what we really need to compute are the probabilities for either planet to have rain on less than 50\\% of all days. To do so let us first determine the likelihoods as a function of the fraction $r$ of rainy days:\n",
    "\n",
    "* $\\mathcal{L}_A(r)=P(\\text{2 of 3 rainy days}|r) = {3\\choose2} r^2(1-r)$\n",
    "* $\\mathcal{L}_B(r)=P(\\text{6 of 10 rainy days}|r) = {10\\choose6} r^6(1-r)^4$\n",
    "\n",
    "Assuming a flat prior on $r$, the posterior probability for the planets to have $r$ rainy days is proportional to these likelihoods, i.e.:\n",
    "\n",
    "* $P_A(r|D) = \\frac{\\mathcal{L}_A(r)}{\\int_0^1 \\mathcal{L}_A(r')dr'}$\n",
    "* $P_B(r|D) = \\frac{\\mathcal{L}_B(r)}{\\int_0^1 \\mathcal{L}_B(r')dr'}$\n",
    "\n",
    "Hence, the posterior probabilities for the planets to be 'good' (r<0.5) are given by\n",
    "\n",
    "* $P(A\\text{ good}|D) = \\int_0^{1/2}P_A(r|D)dr = \\frac{\\int_0^{1/2} r^2(1-r)dr}{\\int_0^1 r^2(1-r)dr} = \\frac{5}{16} = 0.3125$\n",
    "* $P(B\\text{ good}|D) = \\int_0^{1/2}P_B(r|D)dr = \\frac{\\int_0^{1/2} r^6(1-r)^4dr}{\\int_0^1 r^6(1-r)^4dr} = \\frac{281}{1024} = 0.2744141$\n",
    "\n",
    "Neither planet gives us particularly good chances of survival! However, given this analysis, it is clear that **planet A** has be be preferred, given no other information. The odds of planet A versus planet B are $$R=\\frac{P(A\\text{ good}|D)}{P(B\\text{ good}|D)}\\approx1.14.$$\n",
    "\n",
    "In **R**, we can forgo the analytical calculation and do everything numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "likelihood.A = function(r) dbinom(2,3,r)\n",
    "likelihood.B = function(r) dbinom(6,10,r)\n",
    "\n",
    "posterior.A = function(r) likelihood.A(r)/integral(likelihood.A,0,1)\n",
    "posterior.B = function(r) likelihood.B(r)/integral(likelihood.B,0,1)\n",
    "\n",
    "P.A.good = integral(posterior.A,0,0.5)\n",
    "P.B.good = integral(posterior.B,0,0.5)\n",
    "\n",
    "print(P.A.good)\n",
    "print(P.B.good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sharpen our intuition for this problem let us plot the posteriors $P_A(r|D)$ and $P_B(r|D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Rmd_chunk_options": "out.width='70%', fig.align='center'",
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "magcurve(posterior.A(x),xlim=c(0,1),ylim=c(0,3),xaxs='i',yaxs='i',\n",
    "         xlab='Fraction of rainy days r',ylab='Posterior P(r|data)',col='red')\n",
    "curve(posterior.B(x),col='blue',add=T)\n",
    "abline(v=c(2/3,6/10),col=c('red','blue'),lty=2)\n",
    "r = seq(0,0.5,length=100)\n",
    "polygon(c(r,rev(r)),c(posterior.A(r),r*0),col='#ff000030',border=NA)\n",
    "polygon(c(r,rev(r)),c(posterior.B(r),r*0),col='#0000ff30',border=NA)\n",
    "text(0.24,0.6,'Planet A',pos=2,col='red')\n",
    "text(0.47,2,'Planet B',pos=2,col='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that while the mode of the posterior for planet B ($r=6/10=0.6$) is lower than for planet A ($r=2/3\\approx0.667$), the integrated probability of good days ($r<0.5$) is nonetheless higher for planet A. Graphically, this can be seen from the fact that the shaded red region has a larger surface area than the shaded blue region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum likelihood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often one is interested in a *point estimator*, that is a single model parameter $\\hat\\theta$ which 'best' describes the data in some sense. The most commonly used point estimator is the so-called *maximum likelihood estimator (MLE)*, defined as the parameter $\\theta$ that maximises the likelihood:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\hat\\theta_{\\rm MLE} = \\underset{\\theta}{\\arg\\max}\\,\\mathcal{L}(\\theta|D).\n",
    "  \\label{eq:MLE}\n",
    "\\end{equation}\n",
    "\n",
    "The MLE owes its importance to the fact, that it represents the most probable model parameter(s), given a flat prior, $P(\\theta)=\\rm const$, as a direct consequence of Bayes theorem. As detailed by Kendall \\& Stuart (1979, ``The Advanced Theory of Statistics''), the MLE has three remarkable asymptotic properties which make it a powerful statistical tool. As the sample size becomes increasingly large, the MLE becomes a *consistent* (i.e. its expectation converges),  *minimum-variance estimator* and *normally distributed estimator*.\n",
    "\n",
    "It is generally convenient to work with its natural logarithm, called the *log-likelihood*,\n",
    "\\begin{equation}\n",
    "  \\ell(\\theta;D)\\equiv\\ln\\mathcal{L}(\\theta;D).\n",
    "\\end{equation}\n",
    "The logarithm conveniently transforms the product of probabilities into sums of log-probabilities.\n",
    "\n",
    "Since the logarithm is a monotonic function, a maximum of $\\mathcal{L}$ is also a maximum of $\\ell$ and vice versa. Hence, $\\hat\\theta_{\\rm MLE}$ can be obtained either through maximising $\\mathcal{L}$ or $\\ell$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: fitting a black-body spectrum\n",
    "\n",
    "Let us now consider an example of a multivariate inference problem, i.e. a problem where we aim to constrain multiple (here two) parameters simultaneously from the same data.\n",
    "\n",
    "A thermal source of light is observed in three different colour filters. For simplicity, these filters are assumed to be perfect narrow band filters, which are fully transparent in a small wavelength range $\\Delta\\lambda=0.1\\rm nm$, centred at a wavelength $\\lambda_i$, and totally opaque at all other wavelengths. The data obtained through the three filters is summarised in Tab. 2.\n",
    "\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\begin{tabular}{lllc}\n",
    "\\hline\n",
    "Colour & Wavelength $\\lambda_i$ & Photon counts $x_i$ \\\\ \\hline\n",
    "UV & 281 & 9 \\\\\n",
    "blue & 446 & 18 \\\\\n",
    "red & 641 & 11 \\\\ \\hline\n",
    "\\end{tabular}\n",
    "\\caption{Narrow band filter data used for black-body SED fitting.}\n",
    "\\end{table}\n",
    "\n",
    "Assuming a black-body, the energy radiated per unit wavelength over the duration of the measurement, the so-called `spectral energy distribution' (SED), is given by the Planck law\n",
    "\\begin{equation}\n",
    "  F(\\lambda;T,A) =\\frac{10^{-40}{\\rm J m^4}A}{\\lambda^5}\\frac{1}{ e^{\\frac{hc}{\\lambda k_{\\rm B}T}} - 1}~~\\rm[J/m],\n",
    "\\end{equation}\n",
    "where $h$ is the Planck constant, $c$ is the speed of light and $k_{\\rm B}$ is the Boltzmann constant. The model has two parameters: the temperature $T$ and an amplitude $A$, which depends on the geometry and the quantum efficiency of the system, as well as on the duration of the measurement. We assume that $A$ is independent of the wavelength. The constant $10^{-40}\\rm J m^4$ is an arbitrary scaling factor to make $A$ dimensionless and avoid very small values, which could lead to some unnecessary numerical difficulties. Our goal is to determine the most likely values of $T$ and $A$ and their uncertainties. Throughout this exercise we assume that the spectrum of the thermal source is constant over the small bandpass $\\Delta\\lambda$ of the filters.\n",
    "\n",
    "Let's first write down the likelihood for a single narrow band filter $i$: The probability $P_i$ of observing $x_i$ photons at wavelength $\\lambda_i\\pm\\Delta\\lambda/2$ is given by the Poisson statistics $$\\mathcal{L}_i(T,A;x_i) = P_i(x_i|T,A) = \\frac{k_i^{x_i}e^{-k_i}}{x_i!},$$ where $k_i$ is the expected number of photons (of energy $E=hc/\\lambda_i$) seen through the filter centred at $\\lambda_i$, $$k_i=\\Delta\\lambda\\cdot F(\\lambda_i;T,A)\\cdot\\frac{\\lambda_i}{hc},$$ which is dimensionless as required.\n",
    "\n",
    "Hence, the total log-likelihood function becomes\n",
    "\\begin{equation}\n",
    "  \\ell(T,A) = \\sum_{i=1}^3 \\left[x_i\\ln(k_i)-k_i-\\ln(x_i!)\\right],\n",
    "\\end{equation}\n",
    "where the terms $\\ln(x_i!)$ do not depend on the model parameters $T$ and $A$. Therefore, the maximum point and the derivatives of $\\ell$ are the same as those of the effective log-likelihood\n",
    "\\begin{equation}\\label{eq:loglspect}\n",
    "  \\tilde\\ell(T,A) = \\sum_{i=1}^3 \\left[x_i\\ln(k_i)-k_i\\right].\n",
    "\\end{equation}\n",
    "\n",
    "It is possible to make some analytical progress towards finding the maximum point $(\\hat T,\\hat A)$ of $\\tilde\\ell(T,A)$. However, Eq. (\\ref{eq:loglspect}) lends itself to numerical optimisation. In **R**, this becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# initialise data and constants\n",
    "data.wavelength = c(281,446,641) # [nm] values of lambda_i\n",
    "data.counts = c(9,18,11) # values of x_i\n",
    "c = 299792458 # [m/s] speed of light\n",
    "h = 6.62607004e-34 # [m^2 kg/s] Planck's constant\n",
    "k = 1.38064852e-23 # [m^2 kg/s^2/K] Boltzmann constant\n",
    "dlambda = 1e-10 # [m] filter band width\n",
    "\n",
    "# Planck law in energy/wavelength [J/m]\n",
    "planck = function(lambda,temperature,A) {\n",
    "  return(1e-40*max(0,A)*lambda^(-5)/(exp(h*c/(lambda*k*temperature))-1))\n",
    "}\n",
    "\n",
    "# Planck law in photons/Delta lambda\n",
    "planck.photons = function(lambda,temperature,A) {\n",
    "  dlambda*lambda/h/c*planck(lambda,temperature,A)\n",
    "}\n",
    "\n",
    "# likelihood\n",
    "log.likelihood = function(p) { # p = (temperature,A)\n",
    "  k = planck.photons(data.wavelength*1e-9,p[1],p[2])\n",
    "  return(sum(data.counts*log(k+1e-99)-k)) # the constant 1e-99 avoids problems if k=0\n",
    "}\n",
    "\n",
    "# maximise likelihood\n",
    "p.initial = c(1e4,10)\n",
    "fit = optim(p.initial, log.likelihood, hessian=TRUE, control=list(fnscale=-1))\n",
    "T.mle = fit$par[1]\n",
    "A.mle = fit$par[2]\n",
    "cat(sprintf('MLE solution: T = %.3eK, A = %.3e\\n',T.mle,A.mle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the in-built **optim** function to maximise the likelihood. A detailed discussion of this function was given in part 2 of this course.\n",
    "\n",
    "Let us now plot the most likely SED together with the data points and their Poisson uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Rmd_chunk_options": "out.width='70%', fig.align='center'",
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "lambda.nm = seq(1,1000,by=2) # [nm]\n",
    "magplot(lambda.nm,dlambda*lambda.nm*1e-9/c/h*planck(lambda.nm*1e-9,T.mle,A.mle),\n",
    "        pch=20,cex=0.5,ylim=c(0,25),col=wavelength2col(lambda.nm),\n",
    "        xlab='Wavelength [nm]', ylab='Photon counts')\n",
    "points(data.wavelength,data.counts,pch=20,cex=1.5) # data points\n",
    "segments(data.wavelength,\n",
    "         y0=qpois(0.16,data.counts),y1=qpois(0.84,data.counts)) # 16%-84% uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model parameter uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to the problem of estimating the statistical uncertainty of the MLE.\n",
    "\n",
    "In general, $\\ell(\\theta;\\mathbf{x})$ can be a complicated function of $\\theta$. However, it is generally true that the lowest-order non-vanishing approximation of its shape around the absolute maximum $\\hat\\theta_{\\rm MLE}$ is the second order term in the Taylor expansion. If $\\theta$ is a single parameter,\n",
    "\\begin{equation}\\label{eq:loglexpainsion}\n",
    "  \\ell(\\theta;\\mathbf{x}) = \\ell(\\hat\\theta_{\\rm MLE};\\mathbf{x})-\\tfrac{1}{2}\\sigma^{-2}(\\theta-\\hat\\theta_{\\rm MLE})^2+\\mathcal{O}\\big((\\theta-\\hat\\theta_{\\rm MLE})^3\\big),\n",
    "\\end{equation}\n",
    "where, for the moment, $-\\sigma^{-2}$ is just a funny way of writing the second-order Taylor coefficient, computed as\n",
    "\\begin{equation}\n",
    "  -\\sigma^{-2} = \\left.\\frac{d^2\\ell{(\\theta;\\mathbf{x})}}{d\\theta^2}\\right|_{\\theta=\\hat\\theta_{\\rm MLE}}\n",
    "\\end{equation}\n",
    "\n",
    "Since the second derivative at the maximum point is always negative, $\\sigma$ is a well-defined positive real. This parabolic approximation of $\\ell$ corresponds to the Gaussian approximation\n",
    "\\begin{equation}\n",
    "  \\mathcal{L}(\\theta;\\mathbf{x}) \\approx \\mathcal{L}(\\hat\\theta_{\\rm MLE};\\mathbf{x})\\exp\\left(-\\frac{(\\theta-\\hat\\theta_{\\rm MLE})^2}{2\\sigma^2}\\right)\n",
    "\\end{equation}\n",
    "with standard deviation $\\sigma$ (hence use of the symbol $-\\sigma^{-2}$ in Eq. \\ref{eq:loglexpainsion}). By normalising this Gaussian, we obtain the normal parameter PDF (assuming a flat prior $P(\\theta)=\\rm constant$),\n",
    "\\begin{equation}\n",
    "  P(\\theta|\\mathbf{x}) \\approx \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(\\theta-\\hat\\theta_{\\rm MLE})^2}{2\\sigma^2}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "This approximation of the model uncertainty is known as the *Laplace approximation*. This normal approximation of $\\mathcal{L}$ often works surprisingly well in real examples as a consequence of the CLT.\n",
    "\n",
    "The extension of the Laplace approximation to multivariate likelihood functions, i.e. where $\\theta$ is a vector of parameters, is straightforward. In this case the covariance of the model parameters at the MLE solution is given by\n",
    "\\begin{equation}\n",
    "  \\Sigma = -\\mathcal{H}(\\hat\\theta_{\\rm MLE})^{-1},\n",
    "\\end{equation}\n",
    "where $\\mathcal{H}$ is the *Hessian matrix*,\n",
    "\\begin{equation}\n",
    "  \\mathcal{H}_{ij}(\\theta) = \\frac{\\partial^2\\ell{(\\theta;\\mathbf{x})}}{\\partial\\theta_i\\partial\\theta_j}.\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\mathcal{H}(\\hat\\theta_{\\rm MLE})$ is a symmetric negative-definite matrix, the covariance exists and is a symmetric and positive-definite matrix, as it must be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: fitting a black-body spectrum (continued)\n",
    "\n",
    "The parameter covariance in the Laplace approximation can be obtained from the Hessian, which is computed directly by **optim** if the argument *hessian=TRUE* is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Rmd_chunk_options": "out.width='70%', fig.align='center'",
    "kernel": "R",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# plot MLE solution\n",
    "xlim = c(5e3,1.3e4) # needed again later\n",
    "ylim = c(0,2.5e3) # needed again later\n",
    "magplot(T.mle,A.mle,xlim=xlim,ylim=ylim,pch=20,xaxs='i',yaxs='i',\n",
    "        xlab='Temperature T [K]', ylab='Amplitude A')\n",
    "\n",
    "# Laplace approximation\n",
    "covariance = -solve(fit$hessian)\n",
    "lines(ellipse(covariance,centre=c(T.mle,A.mle),level=0.68)) # 1-sigma region\n",
    "lines(ellipse(covariance,centre=c(T.mle,A.mle),level=0.95),lty=2) # 2-sigma region\n",
    "T.sd = sqrt(covariance[1,1])\n",
    "A.sd = sqrt(covariance[2,2])\n",
    "segments(T.mle-T.sd, A.mle, T.mle+T.sd)\n",
    "segments(T.mle, A.mle-A.sd, y1=A.mle+A.sd)\n",
    "\n",
    "# Full posterior\n",
    "n.grid = 150\n",
    "x.range = seq(xlim[1],xlim[2],len=n.grid)\n",
    "y.range = seq(ylim[1],ylim[2],len=n.grid)\n",
    "z = array(NA,c(n.grid,n.grid))\n",
    "for (i in 1:n.grid) {\n",
    "  for (j in 1:n.grid) z[i,j] = log.likelihood(c(x.range[i],y.range[j]))\n",
    "}\n",
    "z = exp(z)\n",
    "contour(x.range,y.range,z,levels=contourlevel(z,c(0.68,0.95)), lty=c(1,2),\n",
    "        drawlabels=FALSE, add=TRUE, col='magenta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 2-$\\sigma$ ellipse of the Laplace approximation reaches to $A<0$, which is clearly unphysical. For more accurate posteriors, we need to consider the full likelihood and identify the minimal regions that contain 68\\% and 95\\% of the probability mass, respectively. In this example, this can be done numerically on a grid (magenta). In general, higher-dimensional problems, more sophisticated techniques are required (e.g. MCMC), which usually produce a sample representing the posterior (see later in this course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imortant resources"
   ]
  }
 ],
 "metadata": {
  "Rmd_chunk_options": {
   "author": "Danail Obreschkow, Harley Wood School of Astronomy",
   "classoption": "a4paper",
   "date": "`r Sys.Date()`",
   "output": "pdf_document",
   "title": "Astrostatistics"
  },
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "python"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ],
    [
     "R",
     "ir",
     "",
     ""
    ]
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
